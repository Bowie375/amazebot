# amazebot
---

## Features:
- SLAM
- navigation
- voice recognition

## How To Run the Code

### Setting up the Environment
1. Clone the repository
```
git clone https://github.com/Bowie375/amazebot.git
```
2. Install submodules
```
git submodule update --init
git submodule update --remote
```

3. Be Sure Ignition Gazebo and RViz2 are correctly Installed

4. Open a terminal and navigate to the cloned repository

5. Build navigation package
```
colcon build --packages-select=navigation
```

6. Run the following command to set environment variables
```
source setup.bash
```

### Visualizing amazeworld in Ignition Gazebo
1. Run the following command to launch the amazebot world
```
ros2 launch navigation bringup_amazeworld.launch.py
```

2. You should see both Ignition Gazebo launched if all previous steps run successfully

3. On another terminal, navigate to the cloned repository and run the following command to maniulate the robot
```
ign topic -t /model/amazebot/cmd_vel -m ignition.msgs.Twist -p "linear: {x: 0.5}"
```
you can see the robot moving forward in Gazebo. And if you want to stop the robot, change the value of x to 0 and run the command again.

4. If you want to manipulate the robot using keyboard, run the following command on a new terminal
```
ros2 run teleop_twist_keyboard teleop_twist_keyboard --ros-args -r cmd_vel:=/model/amazebot/cmd_vel
```

### Running Mapping Using Cartographer
1. run the following command to launch the cartographer node
```
ros2 launch navigation bringup_cartographer.launch.py
```

### Running the Chatting Feature
1. open a new terminal and run the following command to launch the microphone node
'''
source setup.bash
python3 $(AMAZEBOT_BASE_DIR)/src/navigation/navigation/start_microphone.py
'''

2. hit 'r' on the keyboard to start recording the voice

3. when you are done, hit 'q' on the keyboard to stop recording

4. if there is no prompt that tells you recoding is done, wait for a few seconds and hit 'q' again.

5. you can here the response from the robot if everything is set up correctly.

### Running the Navigation Feature
1. run the following command to launch the nav2 node
```
ros2 launch navigation bringup_nav2.launch.py
```

2. assign an initial pose to the robot 
    - click "2D Pose Estimation" label on the top panel of RViz2, choose a point on the map, and drag the point to the direction that the robot faces.

3. tell nav2 the destination position
    - click "Nav2 Goal" label on the top panel of RViz2, choose a point on the map as the destination point and drag it to the destination direction.

4. nav2 will plan the trajectory and navigation to the destination by itself!

## File Organization
This directory is created using 
    ```
    ros2 pkg create --build-type ament_python --license Apache-2.0  navigation
    ```
So, the file organization is as actually pre-defined. What I added to the package are the followings (under ./src/navigation):
- launch/
    - bringup_amazeworld.launch.py: This file launches Ignition Gazebo, and the state publisher node.
    - bringup_cartographer.launch.py: This file launches the cartographer node.
    - bringup_nav2.launch.py: This file launches the nav2 node.

- models/
    - amazebot.sdf : If you want to modify the robot's appearance, you can modify this file.

- worlds/
    - amazeworld.sdf : this is the world file that contains the robot and the environment. You can modify this file to add or remove objects from the environment.

- navigation/
    - sensor_tf_publisher.py : this file publishes the transformation of gpu-lidar and imu to the tf tree.
    - odom_tf_publisher.py : this file published the /odom frame to the tf tree
    - cmd_vel_publisher.py : this file converts the /cmd_vel topic published by nav2 to the /model/amazebot/cmd_vel topic that can be used by Ignition Gazebo.
    - start_microphone.py : this file starts the microphone node that listens to voice commands and plays back the response.
    - start_kimi.py: this file starts the kimi node that uses LLM to process the voice command.

- maps/
    - amaze_map.pgm : this file contains the map generated by cartographer.
    - amaze_map.yaml : this file contains the metadata of the map.

- rviz/
    - cartographer.rviz : this file contains the configuration of RViz2 for cartographer.
    - nav2.rviz : this file contains the configuration of RViz2 for nav2.

For other directories under the src directory, both cartographer, cartographer_ros, and nav2 are third-party packages. And xunfei is a module for voice recognition, I downloaded it from [here](https://www.xfyun.cn/sdk/dispatcher).

## Issues
- Mapping failed to run using odometry.

## References
- [Setting up a Robot Simulation(Gazebo)](https://docs.ros.org/en/humble/Tutorials/Advanced/Simulators/Gazebo/Gazebo.html)
- [Move the Robot](https://gazebosim.org/docs/latest/moving_robot/)
- [Use RViz2 to visualize the Robot](https://d2lros2foxy.fishros.com/#/humble/chapt8/get_started/2.RVIZ2%E5%8F%AF%E8%A7%86%E5%8C%96URDF%E6%A8%A1%E5%9E%8B)
- [Some Useful Ignition Commands (Organized by Myself)](cmd_reminder.md)
